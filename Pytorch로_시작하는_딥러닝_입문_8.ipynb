{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlGoB1X8KekEpKtJiySY9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hansol03/PyTorch-Wikidocs-/blob/main/Pytorch%EB%A1%9C_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%9E%85%EB%AC%B8_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**10.순환 신경망(Recurrent Neural Network)**"
      ],
      "metadata": {
        "id": "8JOrvidl7jbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 파이썬으로 RNN 구현하기 \n",
        "# 아래의 코드는 의사 코드 (pseudocode)로 실제 동작하는 코드가 아님 \n",
        "\n",
        "hidden_state_t = 0 # 초기 은닉 상태를 0(벡터)로 초기화 \n",
        "for input_t in input_length: # 각 시점마다 입력을 받는다 \n",
        "    output_t = tanh(input_t, hidden_state_t) # 각 시점에 대해서 입력과 은닉 상태를 가지고 연산 \n",
        "    hidden_state_t = output_t # 계산 결과는 현재 시점의 은닉 상태가 된다 "
      ],
      "metadata": {
        "id": "rv7KLGe9CyDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "timesteps = 10   #시점의 수. NLP에서는 보통 문장의 길이가 된다. \n",
        "input_size = 4   #입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다. \n",
        "hidden_size = 8   #은닉 상태의 크기. 메모리 셀의 용량이다. \n",
        "\n",
        "inputs = np.random.random((timesteps, input_size)) #입력에 해당되는 2D 텐서 \n",
        "\n",
        "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화 \n",
        "# 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬 "
      ],
      "metadata": {
        "id": "SxYh4l3V-URP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hidden_state_t) # 8의 크기를 가지는 은닉 상태. 현재는 초기 은닉 상태로 모든 차원이 0의 값을 가짐. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhwVx8NC-964",
        "outputId": "51410b10-2b1d-4d7c-8819-28ffdd5f534e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Wx = np.random.random((hidden_size, input_size)) #(8,4) 크기의 2D 텐서 생성. 입력에 대한 가중치 \n",
        "Wh = np.random.random((hidden_size, hidden_size)) #(8,8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치 \n",
        "b = np.random.random((hidden_size,)) # (8,) 크기의 1D 텐서 생성. 이 값은 편향(bias) "
      ],
      "metadata": {
        "id": "ybUDnAup_FKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(Wx)) \n",
        "print(np.shape(Wh)) \n",
        "print(np.shape(b)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOzjIJxn_fFH",
        "outputId": "4684fe72-049c-4350-a5ea-09812fbf530c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 4)\n",
            "(8, 8)\n",
            "(8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_hidden_states = [] \n",
        "\n",
        "# 메모리 셀 동작 \n",
        "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨 \n",
        "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias) \n",
        "    total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적 \n",
        "    print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim) \n",
        "    hidden_state_t = output_t \n",
        "\n",
        "total_hidden_states = np.stack(total_hidden_states, axis = 0) \n",
        "# 출력 시 값을 깔끔하게 해준다. \n",
        "\n",
        "print(total_hidden_states) # (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R5yYlaDBLfG",
        "outputId": "c7f298e7-78db-44ec-c037-ccc77dae661f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "(2, 8)\n",
            "(3, 8)\n",
            "(4, 8)\n",
            "(5, 8)\n",
            "(6, 8)\n",
            "(7, 8)\n",
            "(8, 8)\n",
            "(9, 8)\n",
            "(10, 8)\n",
            "[[0.95901259 0.95487794 0.86839032 0.88284809 0.98961044 0.97589947\n",
            "  0.94790078 0.96294715]\n",
            " [0.99992464 0.99993898 0.99795967 0.9981817  0.99999487 0.9999366\n",
            "  0.9991409  0.99990779]\n",
            " [0.99998601 0.99999137 0.99918635 0.99958979 0.99999922 0.99999247\n",
            "  0.99974652 0.99997534]\n",
            " [0.99998908 0.99999208 0.99957852 0.99953286 0.99999951 0.99999296\n",
            "  0.99982151 0.99998536]\n",
            " [0.9999618  0.99998162 0.99890976 0.99857148 0.99999828 0.99997415\n",
            "  0.99970139 0.99996166]\n",
            " [0.99997285 0.99998298 0.99846931 0.99943038 0.99999831 0.99998419\n",
            "  0.99948338 0.99994924]\n",
            " [0.99998353 0.99999319 0.99957853 0.99896946 0.99999947 0.99998978\n",
            "  0.99990594 0.99998522]\n",
            " [0.99995807 0.99997468 0.99854815 0.99904754 0.99999797 0.99996991\n",
            "  0.99943461 0.99993543]\n",
            " [0.99999191 0.99999314 0.99959458 0.99973905 0.99999962 0.99999495\n",
            "  0.99979133 0.99998414]\n",
            " [0.99998079 0.99998053 0.99973159 0.99929071 0.99999954 0.99997727\n",
            "  0.99970162 0.99997387]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 파이토치의 nn.RNN()\n",
        "import torch\n",
        "import torch.nn as nn "
      ],
      "metadata": {
        "id": "JRUuqNrgCGep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 5 #입력의 크기 \n",
        "hidden_size = 8 #은닉 상태의 크기 "
      ],
      "metadata": {
        "id": "1N32FDQofU-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (batch_size, time_steps, input_size) \n",
        "inputs = torch.Tensor(1, 10, 5) "
      ],
      "metadata": {
        "id": "X_v5lO7Rfdy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell = nn.RNN(input_size, hidden_size, batch_first=True) "
      ],
      "metadata": {
        "id": "Vt89hXL5foYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, _status = cell(inputs)"
      ],
      "metadata": {
        "id": "akyNhc22fy-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.shape) # 모든 time-steps의 hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3oXeedPf7-c",
        "outputId": "eec6a17a-c205-48c6-859d-3b623d1d3ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(_status.shape) # 최종 itme-step의 hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsxTmbkygT4k",
        "outputId": "f3d6231f-a678-4004-c04b-e3655432352b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (batch_size, time_steps, imput_size) \n",
        "inputs = torch.Tensor(1, 10, 5) "
      ],
      "metadata": {
        "id": "zUUNal2iggE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers =2, batch_first = True)"
      ],
      "metadata": {
        "id": "NHrEcPjjik3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, _status = cell(inputs) "
      ],
      "metadata": {
        "id": "bb68oW8JjF-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.shape) # 모든 time-step의 hidden_state "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1T0pO2QiuM_",
        "outputId": "fd43b664-9699-482c-be9c-2eca721f1db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(_status.shape) #(층의 개수, 배치 크기, 은닉 상태의 크기)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxeuP0xliyTn",
        "outputId": "e03fcebc-5152-473e-83bd-6d1bff167797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (batch_sizem time_step, input_size) \n",
        "inputs = torch.Tensor(1, 10, 5) "
      ],
      "metadata": {
        "id": "RYZyFnfZi6Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers =2, batch_first=True, bidirectional = True)"
      ],
      "metadata": {
        "id": "r2qCoKsBkUA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, _status = cell(inputs) "
      ],
      "metadata": {
        "id": "XQKTh9M9kgsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.shape) #(배치크기, 시퀀스 길이, 은닉 상태의 크기 x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jebfye_knlq",
        "outputId": "a0c934ac-b925-469f-93d7-af7b5807e384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(_status.shape) #(층의 개수 x2, 배치 크기, 은닉 상태의 크기)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbKuTc8pkwqs",
        "outputId": "23efa57d-5791-4c49-c3fb-e86880af43d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 파이토치의 nn.LSTM()\n",
        "nn.RNN(input_dim, hidden_size, batch_first=True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "5_1wS-YNk-7_",
        "outputId": "610acaae-5836-467c-def2-1dcec5c64582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0c704d45ccb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 4. 파이토치의 nn.LSTM()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.LSTM(input_dim, hidden_size, batch_first = True) "
      ],
      "metadata": {
        "id": "hTJVqCD4rHcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**11.다대다 RNN을 이용한 텍스트 생성**"
      ],
      "metadata": {
        "id": "rcQw3l_nsEdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.문자 단위 RNN(Char RNN) \n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim\n",
        "import numpy as np "
      ],
      "metadata": {
        "id": "u6Sv8_t-sHXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 훈련 데이터 전처리하기 \n",
        "input_str ='apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str))) \n",
        "vocab_size = len(char_vocab) \n",
        "print('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHiqti3PtLPm",
        "outputId": "2db5d47a-d52e-4104-c12b-b9bf40ab907e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기 \n",
        "hidden_size = 5 \n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "iGTV-s6XteqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) #문자에 고유한 정수 인덱스 부여 \n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1bU1ZQSyVeH",
        "outputId": "97806601-0b3a-4e0d-dfff-f542d33635e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char = {} \n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2PefkCjyeS-",
        "outputId": "cc42cf6a-2ac5-4ed3-ea4d-758a78f9577d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str] \n",
        "y_data = [char_to_index[c] for c in label_str] \n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-u2SSPMyrFN",
        "outputId": "014a6da0-848b-448e-fbe0-96ffc2c2cd23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 차원 추가 \n",
        "# 텐서 연산인 unsqueeze(0) 를 통해 해결할 수도 있었음. \n",
        "x_data = [x_data] \n",
        "y_data = [y_data] \n",
        "print(x_data) \n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XfkEsddy4eJ",
        "outputId": "114be770-2ad2-482d-946b-52f6d6ff1d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guqoBdXuzHnV",
        "outputId": "52361dd1-3d4d-490f-be93-52888681cae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot) \n",
        "Y = torch.LongTensor(y_data) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYRuQ9uWzPW3",
        "outputId": "ed5ab6a6-7b33-423e-cc08-4d376ca4ca8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기: {}'.format(X.shape)) \n",
        "print('레이블의 크기: {}'.format(Y.shape)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt9RlsyCzaFx",
        "outputId": "9cd3d4b5-c3b0-4582-e313-18f10a2d0b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기: torch.Size([1, 5, 5])\n",
            "레이블의 크기: torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 모델 구현하기 \n",
        "class Net(torch.nn.Module): \n",
        "    def __init__(self, input_size, hidden_size, output_size): \n",
        "        super(Net, self).__init__() \n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) #RNN 셀 구현 \n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현 \n",
        "\n",
        "    def forward(self, x): #구현한 RNN 셀과 출력층을 연결 \n",
        "        x, _status = self.rnn(x) \n",
        "        x = self.fc(x) \n",
        "        return x "
      ],
      "metadata": {
        "id": "ToNJ90atzh-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size) "
      ],
      "metadata": {
        "id": "as7CUyyj0IIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X) \n",
        "print(outputs.shape) #3차원 텐서 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGoJGibT0Mju",
        "outputId": "bb96f92f-8379-4c6d-9b2b-c9066977f5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPN8n5Ae0Rbq",
        "outputId": "56cb4715-c77a-4f39-cafc-a250a1007bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7_7WRwO0dRS",
        "outputId": "dcb537de-23b7-451c-8a0e-070e3f263cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(net.parameters(), learning_rate) "
      ],
      "metadata": {
        "id": "xaLFOTT10oAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100): \n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) \n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해 \n",
        "    loss.backward() # 기울기 계산 \n",
        "    optimizer.step() #아까 optimizer 선언 시 넣어둔 파라미터 업데이트 \n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드 \n",
        "    result = outputs.data.numpy().argmax(axis=2) #최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택 \n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)]) \n",
        "    print(i, 'loss:', loss.item(), 'prediction: ', result, 'true Y: ', y_data, 'prediction str: ', result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAxJWrps0yTs",
        "outputId": "f2314125-33b4-49fe-ee50-8ee6ebbabdbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss: 1.629637360572815 prediction:  [[4 0 2 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  p!epp\n",
            "1 loss: 1.354446291923523 prediction:  [[4 4 2 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppeee\n",
            "2 loss: 1.1438581943511963 prediction:  [[4 4 3 3 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppllp\n",
            "3 loss: 0.9643559455871582 prediction:  [[4 4 3 3 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppll!\n",
            "4 loss: 0.77979576587677 prediction:  [[4 4 3 3 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppll!\n",
            "5 loss: 0.6285192370414734 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss: 0.5028437376022339 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss: 0.39548078179359436 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss: 0.3268114924430847 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss: 0.2600756883621216 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss: 0.20321881771087646 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss: 0.15683192014694214 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss: 0.11198900640010834 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss: 0.08023089170455933 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss: 0.05999069660902023 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss: 0.043991535902023315 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss: 0.03162005916237831 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss: 0.023240957409143448 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss: 0.017872357740998268 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss: 0.014285569079220295 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss: 0.011667316779494286 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss: 0.009616008959710598 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss: 0.007962622679769993 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss: 0.006629358045756817 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss: 0.005563217680901289 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss: 0.004716134630143642 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss: 0.004044382832944393 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss: 0.0035103769041597843 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss: 0.0030835180077701807 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss: 0.0027396134100854397 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss: 0.002460188465192914 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss: 0.002231042366474867 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss: 0.0020412937738001347 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss: 0.0018826797604560852 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss: 0.0017488509183749557 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss: 0.001634923042729497 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss: 0.0015369824832305312 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss: 0.0014520391123369336 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss: 0.0013778378488495946 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss: 0.001312360051088035 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss: 0.0012543000048026443 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss: 0.0012024219613522291 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss: 0.001155656878836453 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss: 0.001113362843170762 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss: 0.001074969652108848 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss: 0.00103985914029181 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss: 0.001007770304568112 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss: 0.0009782270062714815 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss: 0.0009509915253147483 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss: 0.0009258741629309952 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss: 0.000902612810023129 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss: 0.0008811128209345043 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss: 0.0008610882796347141 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss: 0.0008424917468801141 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss: 0.0008251330000348389 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss: 0.0008090598275884986 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss: 0.0007940101204439998 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss: 0.000779984169639647 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss: 0.0007667675381526351 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss: 0.000754408014472574 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss: 0.0007428104290738702 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss: 0.0007318080170080066 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss: 0.0007215674850158393 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss: 0.0007118271896615624 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss: 0.000702562858350575 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss: 0.0006938462029211223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss: 0.0006855105748400092 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss: 0.0006775559741072357 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss: 0.0006699584773741663 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss: 0.0006626943359151483 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss: 0.0006557159940712154 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss: 0.0006489757215604186 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss: 0.0006425689207389951 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss: 0.000636376382317394 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss: 0.0006303266854956746 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss: 0.0006245627882890403 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss: 0.0006189416744746268 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss: 0.0006134635186754167 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss: 0.0006081520696170628 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss: 0.0006030072108842432 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss: 0.000597957638092339 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss: 0.0005930747138336301 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss: 0.0005882394034415483 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss: 0.0005835707997903228 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss: 0.0005789974238723516 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss: 0.0005745192756876349 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss: 0.0005701364134438336 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss: 0.0005658011650666595 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss: 0.000561608758289367 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss: 0.0005574639653787017 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss: 0.0005534382071346045 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss: 0.0005494601791724563 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss: 0.0005455057835206389 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss: 0.0005416228668764234 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss: 0.0005378352361731231 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss: 0.0005341666983440518 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss: 0.0005304981023073196 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss: 0.000526853371411562 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss: 0.0005232799449004233 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss: 0.0005197779973968863 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**02. 문자 단위 RNN(Char RNN) - 더 많은 데이터**"
      ],
      "metadata": {
        "id": "VvRvZzwqneTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 문자 단위 RNN(Char RNN) \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.optim as optim "
      ],
      "metadata": {
        "id": "JnlJouTY2HJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 훈련 데이터 전처리하기 \n",
        "\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to\" \n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\") "
      ],
      "metadata": {
        "id": "x8ZO6ccWn0qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence)) #중복을 제거한 문자 집합 생성 \n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩 \n",
        "\n",
        "print(char_dic) # 공백도 여기서는 하나의 원소 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2dFGjOQoKKU",
        "outputId": "7b4a9a7a-bde0-49d3-fd4e-8fe186d36f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d': 0, ',': 1, 'b': 2, 'm': 3, 'i': 4, 'h': 5, 'g': 6, 'o': 7, 'l': 8, 'k': 9, 'f': 10, 'r': 11, 'y': 12, 'n': 13, 'c': 14, 'u': 15, \"'\": 16, 'p': 17, 'w': 18, ' ': 19, 's': 20, 'e': 21, 't': 22, '.': 23, 'a': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic) \n",
        "print('문자 집합의 크기: {}'.format(dic_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfrB_YNXokMq",
        "outputId": "ed90c119-5243-4203-aafb-b709ca90e7d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정 \n",
        "hidden_size = dic_size\n",
        "sequence_length = 10 # 임의 숫자 지정 \n",
        "learning_rate = 0.1 "
      ],
      "metadata": {
        "id": "rGApw5dGpXu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구성 \n",
        "x_data = [] \n",
        "y_data = [] \n",
        "\n",
        "for i in range (0, len(sentence) - sequence_length): \n",
        "    x_str = sentence[i:i + sequence_length] \n",
        "    y_str = sentence[i +1: i+ sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str) \n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str]) # x str to index \n",
        "    y_data.append([char_dic[c] for c in y_str]) # y str to index "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOE2Z569pqjm",
        "outputId": "060981c5-193a-49ac-b3b7-48cf757388fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether toc\n",
            "52 gether toc -> ether toco\n",
            "53 ether toco -> ther tocol\n",
            "54 ther tocol -> her tocoll\n",
            "55 her tocoll -> er tocolle\n",
            "56 er tocolle -> r tocollec\n",
            "57 r tocollec ->  tocollect\n",
            "58  tocollect -> tocollect \n",
            "59 tocollect  -> ocollect w\n",
            "60 ocollect w -> collect wo\n",
            "61 collect wo -> ollect woo\n",
            "62 ollect woo -> llect wood\n",
            "63 llect wood -> lect wood \n",
            "64 lect wood  -> ect wood a\n",
            "65 ect wood a -> ct wood an\n",
            "66 ct wood an -> t wood and\n",
            "67 t wood and ->  wood and \n",
            "68  wood and  -> wood and d\n",
            "69 wood and d -> ood and do\n",
            "70 ood and do -> od and don\n",
            "71 od and don -> d and don'\n",
            "72 d and don' ->  and don't\n",
            "73  and don't -> and don't \n",
            "74 and don't  -> nd don't a\n",
            "75 nd don't a -> d don't as\n",
            "76 d don't as ->  don't ass\n",
            "77  don't ass -> don't assi\n",
            "78 don't assi -> on't assig\n",
            "79 on't assig -> n't assign\n",
            "80 n't assign -> 't assign \n",
            "81 't assign  -> t assign t\n",
            "82 t assign t ->  assign th\n",
            "83  assign th -> assign the\n",
            "84 assign the -> ssign them\n",
            "85 ssign them -> sign them \n",
            "86 sign them  -> ign them t\n",
            "87 ign them t -> gn them ta\n",
            "88 gn them ta -> n them tas\n",
            "89 n them tas ->  them task\n",
            "90  them task -> them tasks\n",
            "91 them tasks -> hem tasks \n",
            "92 hem tasks  -> em tasks a\n",
            "93 em tasks a -> m tasks an\n",
            "94 m tasks an ->  tasks and\n",
            "95  tasks and -> tasks and \n",
            "96 tasks and  -> asks and w\n",
            "97 asks and w -> sks and wo\n",
            "98 sks and wo -> ks and wor\n",
            "99 ks and wor -> s and work\n",
            "100 s and work ->  and work,\n",
            "101  and work, -> and work, \n",
            "102 and work,  -> nd work, b\n",
            "103 nd work, b -> d work, bu\n",
            "104 d work, bu ->  work, but\n",
            "105  work, but -> work, but \n",
            "106 work, but  -> ork, but r\n",
            "107 ork, but r -> rk, but ra\n",
            "108 rk, but ra -> k, but rat\n",
            "109 k, but rat -> , but rath\n",
            "110 , but rath ->  but rathe\n",
            "111  but rathe -> but rather\n",
            "112 but rather -> ut rather \n",
            "113 ut rather  -> t rather t\n",
            "114 t rather t ->  rather te\n",
            "115  rather te -> rather tea\n",
            "116 rather tea -> ather teac\n",
            "117 ather teac -> ther teach\n",
            "118 ther teach -> her teach \n",
            "119 her teach  -> er teach t\n",
            "120 er teach t -> r teach th\n",
            "121 r teach th ->  teach the\n",
            "122  teach the -> teach them\n",
            "123 teach them -> each them \n",
            "124 each them  -> ach them t\n",
            "125 ach them t -> ch them to\n",
            "126 ch them to -> h them to \n",
            "127 h them to  ->  them to l\n",
            "128  them to l -> them to lo\n",
            "129 them to lo -> hem to lon\n",
            "130 hem to lon -> em to long\n",
            "131 em to long -> m to long \n",
            "132 m to long  ->  to long f\n",
            "133  to long f -> to long fo\n",
            "134 to long fo -> o long for\n",
            "135 o long for ->  long for \n",
            "136  long for  -> long for t\n",
            "137 long for t -> ong for th\n",
            "138 ong for th -> ng for the\n",
            "139 ng for the -> g for the \n",
            "140 g for the  ->  for the e\n",
            "141  for the e -> for the en\n",
            "142 for the en -> or the end\n",
            "143 or the end -> r the endl\n",
            "144 r the endl ->  the endle\n",
            "145  the endle -> the endles\n",
            "146 the endles -> he endless\n",
            "147 he endless -> e endless \n",
            "148 e endless  ->  endless i\n",
            "149  endless i -> endless im\n",
            "150 endless im -> ndless imm\n",
            "151 ndless imm -> dless imme\n",
            "152 dless imme -> less immen\n",
            "153 less immen -> ess immens\n",
            "154 ess immens -> ss immensi\n",
            "155 ss immensi -> s immensit\n",
            "156 s immensit ->  immensity\n",
            "157  immensity -> immensity \n",
            "158 immensity  -> mmensity o\n",
            "159 mmensity o -> mensity of\n",
            "160 mensity of -> ensity of \n",
            "161 ensity of  -> nsity of t\n",
            "162 nsity of t -> sity of th\n",
            "163 sity of th -> ity of the\n",
            "164 ity of the -> ty of the \n",
            "165 ty of the  -> y of the s\n",
            "166 y of the s ->  of the se\n",
            "167  of the se -> of the sea\n",
            "168 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UvmlX0uqg0Q",
        "outputId": "4697012f-dbed-45db-bb40-9b187e4558c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 10, 19, 12, 7, 15, 19, 18, 24, 13]\n",
            "[10, 19, 12, 7, 15, 19, 18, 24, 13, 22]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩 \n",
        "X = torch.FloatTensor(x_one_hot) \n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDV6x7nYrGuR",
        "outputId": "1952726a-6a94-4f31-b488-b25879e0f55f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기: {}'.format(X.shape))\n",
        "print('레이블의 크기: {}'.format(Y.shape)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTc1qALesVGv",
        "outputId": "1b54a576-7e57-4826-92ed-764f934124b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기: torch.Size([169, 10, 25])\n",
            "레이블의 크기: torch.Size([169, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOJAvAVDsq8z",
        "outputId": "5acf0a69-82fd-4995-9c0b-18011fb56509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 모델 구현하기 \n",
        "\n",
        "class Net(torch.nn.Module): \n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic)size와 같음 \n",
        "        super(Net, self).__init__() \n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True) \n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True) \n",
        "\n",
        "    def forward(self, x): \n",
        "        x, _status = self.rnn(x) \n",
        "        x = self.fc(x) \n",
        "        return x "
      ],
      "metadata": {
        "id": "-FV0jNmWs6rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두개 쌓습니다. "
      ],
      "metadata": {
        "id": "AvoIF-2CwbbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(net.parameters(), learning_rate) "
      ],
      "metadata": {
        "id": "gdydHR540CLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X) \n",
        "print(outputs.shape) # 3차원 텐서 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHfCNVfC0onT",
        "outputId": "60e1656f-488a-4d01-bbb9-738ee0457a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([169, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VhMAJDj0sp4",
        "outputId": "d133dd6c-b265-410a-fcc4-fee4b3d2ea0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1690, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape) \n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ01Jpn31XQi",
        "outputId": "ceadd19a-44c2-46ef-cf81-a235d9a06e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([169, 10])\n",
            "torch.Size([1690])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100): \n",
        "    optimizer.zero_grad() \n",
        "    outputs = net(X) #(170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용 \n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1)) \n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        "\n",
        "    # results의 텐서 크기는 (170, 10) \n",
        "    results = outputs.argmax(dim=2) \n",
        "    predict_str = \"\" \n",
        "    for j, result in enumerate(results): \n",
        "        if j == 0: #처음에는 예측 결과를 전부 가져오지만 \n",
        "            predict_str += ''.join([char_set[t] for t in result]) \n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가 \n",
        "            predict_str += char_set[result[-1]] \n",
        "\n",
        "    print(predict_str) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNIo7GyD1dJ_",
        "outputId": "d8cdbe64-cd36-41e6-dfec-5561624601b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ddddddddddddddddddddddddddddddddtddodddddddd ddddddddddddddddddddddddddddddddddddtdddddddddddddddddddddddddddddddddddddddddddddddddddddddeddedddddddddddtdddddddddddddddddtddddddd\n",
            "   o   o o  o   o o       o o   o    to o  o          o o            o o   o                    o  o     o     to     o          o       o    o  to   o   o o o      o o  to   o  \n",
            " lstdrdtdrdthtmdtddtddpd mdr. h . drdrdto rdr  rduh . td.trdto tdro ut rdtdt.dtut. drdrdt.stdhdrdtrdrdt. m htm ddrd rdmdrdto mdrdtdd rdt.dtd td .sdtrdrdro rdrdtrdtrrt  rdrdth rdt\n",
            " t    st t thr.r tk tt  t   trtttt .rt thrkt tt ithr t t  tr tt ttrtr    .tr t  dt  drtt  tr t tt  rrtt r tr t t r t t rt t rtr ttrrrtr tt ttf r t  .rt  trr ttt t tt  tr t  tr r \n",
            " tto  to ttto stet  tt ttttt ttttttt t tt tt t tttott  tt ttt  tt tttt  tttttt t t t  t t  t t ttt  tt ettt  t t   ttt tt tte t tott  tttttt ttt te tttt ttt  t  t      t t tttttt\n",
            "  to  woe  eo ben t e ohe   to  t b  e bh toh lh  o  tle  oe      t  o l  o  tonltetbo t  lhen to l  ito tonlt tuh  otlee to lo toee th bo  leo lten  o   e l o lo   o b  toe  o b\n",
            "toso  woe  so ton h toaontn aoo h th o th ths  o to e ee lt to    on o t toa aonlh too tosohe  t th ktoh tonlh toh wosoeo tonth whet t tth 'osos tethso t   heohth o o o  toe so t\n",
            " nton tos  aostoo t aoaoeo  aoo t to t th teo  h tose oed tono    tnbos  toa ton t tne lnstoo  tosos too ton t too tosoeo ton h toe  ton h eiwoa toe so t se ao t d  osto toe wo t\n",
            "d ton tosd toskuo t tnaoe t aoo t ao m tn ten  h tos  oem tod     t wos  tod ton t ane gnstoem toso   os ton t ton ton em toskh toe  todkn e woa toe wo te d to kn  todto toe wort\n",
            "d too tosd thstun d anaoe t tor't to t an terd d to t rem todo k  t bo   tnd tor t tn  lndtnem tost ktnd tos t ton tonhem todth toem tosto didor toemso ke d woskn  t dof toemsosk\n",
            "dnton tond tosbu, d tnaoe t don't toem an terdle dos them todo le d woi  tnd ton't tne doduhem toski tnd tod t tu, tonhem tosk, toem todke ' dor toemdod e e tosk,d endoo toe sor \n",
            "dntooltand toiba, d tnaoe t don t to t to tendle to lther toro le d tool tnd too't too gn ehem toshi tnd tor , ta, tanher tork, toem torbe 'etor the dod e e to 'ud gndo, toe dor \n",
            "dntooltand wo bu, l anaoip, don't do t ty derdli do lther thro le d woop tnd aoo't daoign ther toshi tnd to k, da, tathem tork, toem to de giwor thepend e e ay gnd gn uo thepeorp\n",
            "dntholtand wo bu, l unsoip, don't dout ay terp e do ether thro le p woop and uos't dnsign ther toshs tnd to k, da, wathem tos , them to desg wor the eod e   ty  asign uf the eor \n",
            "pntholwand to bu,ll unsoip, don't aoum tn penp e do ether thro le p woop and toslt dnsign ther tosks tnd to k, dut bather tos , them to bosg wor the sod e s ty  asign uf the sor \n",
            "pntholwant wo buill ansoip, don't aoum to penple to ether thcollecp woop and ton't ansign ther tosks tnd took, but bathem tos h them to bong for the sodle s uyrlnsitn of the sor \n",
            "pathogwant wo butll ansoip, don't aoum tp phnple together thcollect worpland won't ansign them tosks tnd wonk, but tathem tosch them to bong for the sodlens aprensity of the sosc\n",
            "patho want wo luill ansoip, don't aoum tm people thgether thcollect worpland ton't ansign them tosks tnd wo kt but wathem tosch them to bong for the sndlens am ensity of the sosc\n",
            "patho want to luild a soip, don't aoum am people thgethem tocollect woop and ton't ansign them tosks tnd wo k, but tathem tesch them to long for the sndlens amsensity of the sosc\n",
            "patoo want to luild a ship, don't drum ap people together tocollect word and won't ansign ther tosks and wo k, bui wather to ch them to long for the sndlens imsensity of the sosc\n",
            "patoo wont to luild a ship, don't drum ap people together tecollect word and don't dnsign them tosks and dook, but buther tesch them to long for the sndlens im ensity of the sonc\n",
            "t toi want to luild a ship, don't drum ap teodle together tocollect wood and don't dnsign them tosks and dook, but tuther tocch them to long for the sndless im ensity of the sonc\n",
            "t toi want to luild a ship, don't arum ap teodle together terollect wood and don't assign them tosks and dork, but ruther terch them to long for the sndless im ensity of the sonc\n",
            "t too want to build a ship, don't arum ap pentle thgether tocollect wood and don't assign them tosks and dork, but ruther teach them to bong for the snd ess im ensity of the sonc\n",
            "t too want to build a ship, don't drum up peotle together tecollect wood and don't dssign them tosks and dork, but ruther teach them to bong for the endless immensity of the eesc\n",
            "t toa want to luild a ship, don't drum up people together tocollect wood and don't dssign them tosks and dork, but rather teach them to bong for the endless immensity of the eesc\n",
            "t ,oa want to luild a ship, don't drum up people together tocollect wood and don't assign them tosks and dork, but rather teach them to long for the sndless immensity of the sess\n",
            "t ,oa want to luild a ship, don't drum up people together tocollect wood and don't dssign them tosks and dork, but rather teach them to long for the sndless immensity of the sess\n",
            "p yoa want to build a ship, don't arum up people together tocollect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the sesc\n",
            "p yoa want to build a ship, don't drum up people together tocollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "p yoalwant to build a ship, don't drum up people together tocollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "p youlwant to build a ship, don't arum up people together tocollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seat\n",
            "p youlwant to build a ship, don't drum up people together tocollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the seat\n",
            "l youlwant to build a ship, don't arum up people together tocollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seat\n",
            "l you want to build a ship, don't drum up people together tecollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sest\n",
            "l you want to build a ship, don't arum up people together tecollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seat\n",
            "l you want to build a ship, don't arum up people together tecollect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tecollect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tecollect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tecollect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tecollect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tocollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tocollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tocollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together tocollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together tocollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**03. 단어 단위 RNN - 임베딩사용**"
      ],
      "metadata": {
        "id": "eMH53cOS5kmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 훈련 데이터 전처리하기 \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.optim as optim "
      ],
      "metadata": {
        "id": "cKvU6r6e4bL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Repeat is the best medicine for memory'.split() "
      ],
      "metadata": {
        "id": "RhhB030b57tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set(sentence)) \n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1jPn5jo6mea",
        "outputId": "f246808a-e912-4f9f-b77e-c610f37fd689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'Repeat', 'memory', 'medicine', 'for', 'best', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)} # 단어에 고유한 정수 부여 \n",
        "word2index['<unk>']=0 "
      ],
      "metadata": {
        "id": "34dUxWR36p6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW_hyaaq7ok3",
        "outputId": "a4b4547a-dd9c-4035-eaf1-2eac4477ca43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'Repeat': 2, 'memory': 3, 'medicine': 4, 'for': 5, 'best': 6, 'is': 7, '<unk>': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2index['memory'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnY22iZp7zY-",
        "outputId": "eec5dad2-73ec-4dc5-eca6-7c5acb91b699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수치화된 데이터를 단어로 바꾸기 위한 사전 \n",
        "index2word = {v: k for k, v in word2index.items()} \n",
        "print(index2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhzRLH3I74ho",
        "outputId": "dcb5726f-2de9-41d9-dad0-7e57fb40a6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'the', 2: 'Repeat', 3: 'memory', 4: 'medicine', 5: 'for', 6: 'best', 7: 'is', 0: '<unk>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(index2word[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtfvmrMn8NUF",
        "outputId": "be301af6-68d0-4499-d84c-2d3a3d694eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repeat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_data(sentence, word2index): \n",
        "    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환 \n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리 \n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가 \n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가 \n",
        "    return input_seq, label_seq"
      ],
      "metadata": {
        "id": "V3zthnwz8eTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = build_data(sentence, word2index)"
      ],
      "metadata": {
        "id": "E7Uk0rIH9Coo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X) \n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stkUndth9GOi",
        "outputId": "dcc708bf-bc6a-4fbf-ff1c-a88153f2a48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 7, 1, 6, 4, 5]])\n",
            "tensor([[7, 1, 6, 4, 5, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 모델 구현하기 \n",
        "class Net(nn.Module): \n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True): \n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, #워드 임베딩 \n",
        "                                            embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉상태의 크기 정의 \n",
        "                                batch_first=batch_first) \n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함. \n",
        "\n",
        "    def forward(self, x): \n",
        "        # 1. 임베딩 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원) \n",
        "        output = self.embedding_layer(x) \n",
        "        # 2. RNN 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기) , hidden(1, 배치 크기, 은닉층 크기) \n",
        "        output, hidden = self.rnn_layer(output) \n",
        "        # 3. 최종 출력층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기) \n",
        "        output = self.linear(output) \n",
        "        # 4. view를 통해서 배치 차원 제거 \n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
        "        return output.view(-1, output.size(2))"
      ],
      "metadata": {
        "id": "AvrJ2De09H--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters \n",
        "vocab_size = len(word2index) # 단어장 크기는 임베딩층, 최종 출력층에 사용된다. <unk>토큰을 크기에 포함한다.\n",
        "input_size = 5 #임베딩된 차원의 크기및 RNN층 입력 차원의 크기\n",
        "hidden_size = 20 # RNN의 은닉층 크기"
      ],
      "metadata": {
        "id": "7Rr4BEu_BI1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성 \n",
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True) \n",
        "# 손실 함수 정의\n",
        "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도됨\n",
        "# optimizer 정의의\n",
        "optimizer = optim.Adam(params=model.parameters()) "
      ],
      "metadata": {
        "id": "1NBzuxyQBpa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#임의로 예측 해보기. 가중치는 전부 랜덤 초기화된 상태이다. \n",
        "output = model(X) \n",
        "print(output) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxYIgLeTCF66",
        "outputId": "0f9d6989-a0a5-4eb7-f51e-5a094f07f95f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2703,  0.0212, -0.1778, -0.1823, -0.3124, -0.0098,  0.0105,  0.1018],\n",
            "        [ 0.1278,  0.1028, -0.1142, -0.1996, -0.2963, -0.0347, -0.0227, -0.0950],\n",
            "        [ 0.0596, -0.1483, -0.0788,  0.3530, -0.0436, -0.1035,  0.0769,  0.0761],\n",
            "        [-0.2229, -0.0556, -0.1420, -0.9191, -0.4391, -0.2798, -0.1591, -0.1070],\n",
            "        [ 0.0684, -0.1317, -0.1057, -0.6691, -0.5215,  0.0618,  0.0842,  0.1171],\n",
            "        [ 0.1225,  0.0263,  0.0281, -0.4478, -0.4442, -0.1227,  0.0495,  0.0472]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS-kBXqOCXkb",
        "outputId": "0c4f4cde-3372-46d8-d417-d1871843c725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수치화된 데이터를 단어로 전환하는 함수\n",
        "decode = lambda y: [index2word.get(x) for x in y]"
      ],
      "metadata": {
        "id": "PUeIWoKjCpl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 시작\n",
        "for step in range(201): \n",
        "    # 경사 초기화\n",
        "    optimizer.zero_grad() \n",
        "    # 순방향 전파\n",
        "    output = model(X) \n",
        "    # 손실값 계산\n",
        "    loss = loss_function(output, Y.view(-1)) \n",
        "    # 역방향 전파\n",
        "    loss.backward() \n",
        "    # 매개변수 업데이트 \n",
        "    optimizer.step() \n",
        "    # 기록\n",
        "    if step % 40 == 0: \n",
        "        print('[{:02d}/201] {:.4f} '.format(step+1, loss)) \n",
        "        pred = output.softmax(-1).argmax(-1).tolist() \n",
        "        print(' '.join(['Repeat'] + decode(pred))) \n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CADQNLHRCz3l",
        "outputId": "09d75b74-7baf-4385-fcb5-d83f0f707c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/201] 0.1414 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[41/201] 0.0911 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[81/201] 0.0642 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.0482 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.0378 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.0306 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_RMMF2rtDjVL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}